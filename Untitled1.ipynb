{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf4fd0a6-dd8a-4a0a-880c-ff4baa774af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=100, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "epoch:  0 loss:  85.75841522216797\n",
      "epoch:  1 loss:  83.7375259399414\n",
      "epoch:  2 loss:  81.79911041259766\n",
      "epoch:  3 loss:  79.80228424072266\n",
      "epoch:  4 loss:  77.5121078491211\n",
      "epoch:  5 loss:  74.79102325439453\n",
      "epoch:  6 loss:  71.53963470458984\n",
      "epoch:  7 loss:  67.6519546508789\n",
      "epoch:  8 loss:  63.076297760009766\n",
      "epoch:  9 loss:  57.76588821411133\n",
      "epoch:  10 loss:  51.644683837890625\n",
      "epoch:  11 loss:  44.67707824707031\n",
      "epoch:  12 loss:  36.903133392333984\n",
      "epoch:  13 loss:  28.477563858032227\n",
      "epoch:  14 loss:  19.74614715576172\n",
      "epoch:  15 loss:  11.34184741973877\n",
      "epoch:  16 loss:  4.347230911254883\n",
      "epoch:  17 loss:  0.35649147629737854\n",
      "epoch:  18 loss:  1.173837661743164\n",
      "epoch:  19 loss:  6.601728439331055\n",
      "epoch:  20 loss:  12.100554466247559\n",
      "epoch:  21 loss:  13.481314659118652\n",
      "epoch:  22 loss:  11.001201629638672\n",
      "epoch:  23 loss:  6.9084601402282715\n",
      "epoch:  24 loss:  3.1957380771636963\n",
      "epoch:  25 loss:  0.8519298434257507\n",
      "epoch:  26 loss:  0.028546540066599846\n",
      "epoch:  27 loss:  0.3459049165248871\n",
      "epoch:  28 loss:  1.2651304006576538\n",
      "epoch:  29 loss:  2.3297810554504395\n",
      "epoch:  30 loss:  3.223212480545044\n",
      "epoch:  31 loss:  3.769152879714966\n",
      "epoch:  32 loss:  3.903836250305176\n",
      "epoch:  33 loss:  3.6473464965820312\n",
      "epoch:  34 loss:  3.076427459716797\n",
      "epoch:  35 loss:  2.3070342540740967\n",
      "epoch:  36 loss:  1.480037808418274\n",
      "epoch:  37 loss:  0.7434433102607727\n",
      "epoch:  38 loss:  0.22815650701522827\n",
      "epoch:  39 loss:  0.01738925091922283\n",
      "epoch:  40 loss:  0.11683288961648941\n",
      "epoch:  41 loss:  0.4411480128765106\n",
      "epoch:  42 loss:  0.834301769733429\n",
      "epoch:  43 loss:  1.1268805265426636\n",
      "epoch:  44 loss:  1.206070065498352\n",
      "epoch:  45 loss:  1.0579535961151123\n",
      "epoch:  46 loss:  0.758665144443512\n",
      "epoch:  47 loss:  0.4259128272533417\n",
      "epoch:  48 loss:  0.1639697402715683\n",
      "epoch:  49 loss:  0.029150625690817833\n",
      "epoch:  50 loss:  0.023755596950650215\n",
      "epoch:  51 loss:  0.11063063144683838\n",
      "epoch:  52 loss:  0.2354598194360733\n",
      "epoch:  53 loss:  0.34675538539886475\n",
      "epoch:  54 loss:  0.40878549218177795\n",
      "epoch:  55 loss:  0.4069414436817169\n",
      "epoch:  56 loss:  0.34700295329093933\n",
      "epoch:  57 loss:  0.2501540184020996\n",
      "epoch:  58 loss:  0.14538221061229706\n",
      "epoch:  59 loss:  0.06080332770943642\n",
      "epoch:  60 loss:  0.015635311603546143\n",
      "epoch:  61 loss:  0.01487716194242239\n",
      "epoch:  62 loss:  0.048550259321928024\n",
      "epoch:  63 loss:  0.0962042585015297\n",
      "epoch:  64 loss:  0.13526101410388947\n",
      "epoch:  65 loss:  0.14978520572185516\n",
      "epoch:  66 loss:  0.13580630719661713\n",
      "epoch:  67 loss:  0.10105317085981369\n",
      "epoch:  68 loss:  0.05992312356829643\n",
      "epoch:  69 loss:  0.026649197563529015\n",
      "epoch:  70 loss:  0.009931505657732487\n",
      "epoch:  71 loss:  0.010872927494347095\n",
      "epoch:  72 loss:  0.024200333282351494\n",
      "epoch:  73 loss:  0.04147563502192497\n",
      "epoch:  74 loss:  0.05465855076909065\n",
      "epoch:  75 loss:  0.058755841106176376\n",
      "epoch:  76 loss:  0.052929285913705826\n",
      "epoch:  77 loss:  0.04004306718707085\n",
      "epoch:  78 loss:  0.02507644146680832\n",
      "epoch:  79 loss:  0.013051283545792103\n",
      "epoch:  80 loss:  0.0072167981415987015\n",
      "epoch:  81 loss:  0.008079282008111477\n",
      "epoch:  82 loss:  0.013566561974585056\n",
      "epoch:  83 loss:  0.020176293328404427\n",
      "epoch:  84 loss:  0.024565057829022408\n",
      "epoch:  85 loss:  0.0248727947473526\n",
      "epoch:  86 loss:  0.021246878430247307\n",
      "epoch:  87 loss:  0.01544586569070816\n",
      "epoch:  88 loss:  0.009825765155255795\n",
      "epoch:  89 loss:  0.006262259092181921\n",
      "epoch:  90 loss:  0.00549614941701293\n",
      "epoch:  91 loss:  0.007016611751168966\n",
      "epoch:  92 loss:  0.009512616321444511\n",
      "epoch:  93 loss:  0.011545038782060146\n",
      "epoch:  94 loss:  0.012141362763941288\n",
      "epoch:  95 loss:  0.01109764352440834\n",
      "epoch:  96 loss:  0.00892547331750393\n",
      "epoch:  97 loss:  0.006523511838167906\n",
      "epoch:  98 loss:  0.004746364429593086\n",
      "epoch:  99 loss:  0.004057893995195627\n",
      "39.873939514160156\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "img_size = 64\n",
    "data_dir = 'your path to train dir'  # Укажите путь к директории с изображениями\n",
    "\n",
    "# Подготовка данных\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Загрузка данных\n",
    "train_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class SONIKL(nn.Module):\n",
    " def __init__(self):\n",
    "     super().__init__()\n",
    "     self.sec = nn.Sequential(\n",
    "         nn.Conv2d(in_channels=3,out_channels=256, kernel_size= (3,3),padding=1),\n",
    "         nn.ReLU(),\n",
    "         \n",
    "         nn.Conv2d(in_channels=256,out_channels=128, kernel_size= (4,4),padding=1),\n",
    "         nn.ReLU(),\n",
    "         \n",
    "         nn.MaxPool2d((2,2), stride=1),\n",
    "         nn.Conv2d(in_channels=128,out_channels=64, kernel_size= (5,5),padding=1),\n",
    "         nn.ReLU(),\n",
    "         \n",
    "         nn.MaxPool2d((2,2), stride=1),\n",
    "         nn.Conv2d(in_channels=64,out_channels=32, kernel_size= (5,5),padding=1),\n",
    "         nn.ReLU(),\n",
    "         \n",
    "         nn.Conv2d(in_channels=32,out_channels=16, kernel_size= (6,6),padding=1),\n",
    "         nn.ReLU(),\n",
    "         \n",
    "     )\n",
    "     self.flattened_size = self._get_flattened_size()\n",
    "\n",
    "     self.Linear_Stack = nn.Sequential(\n",
    "       nn.Linear(self.flattened_size,64),\n",
    "       nn.ReLU(),\n",
    "       nn.Linear(64,32),\n",
    "       nn.ReLU(),\n",
    "       nn.Linear(32,1),\n",
    "     )\n",
    " def _get_flattened_size(self):\n",
    "        # Создаем временный тензор, чтобы вычислить размер выходов\n",
    "        with torch.no_grad():\n",
    "            x = torch.zeros(1, 3, img_size, img_size)  # Входной тензор\n",
    "            x = self.sec(x)\n",
    "            return x.numel()  # Возвращаем общее количество элементов\n",
    "    \n",
    " def forward(self,x):  \n",
    "    x= self.sec(x)\n",
    "    x = torch.flatten(x, 1)\n",
    "    x=self.Linear_Stack(x)\n",
    "    return x\n",
    "\n",
    "model= SONIKLOX()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "for i in range(10):\n",
    "    for images,labels, in train_loader:\n",
    "        labels = labels.float().view(-1,1)\n",
    "        optimizer.zero_grad()\n",
    "        output= model(images)\n",
    "        loss = criterion(output,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(loss.item())\n",
    "\n",
    "\n",
    "model.eval()  # Устанавливаем модель в режим оценки\n",
    "\n",
    "# Загрузка и преобразование одного изображения\n",
    "not_train_image_path = 'C:\\\\Users\\\\Slay\\\\Desktop\\\\train.jpg'  # Укажите путь к изображению\n",
    "image = Image.open(image_path).convert('RGB')  # Открываем изображение и конвертируем в RGB\n",
    "image = transform(image).unsqueeze(0)  # Применяем преобразования и добавляем размер батча\n",
    "\n",
    "# Прогоняем изображение через модель\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "    probability = torch.sigmoid(output)  # Преобразуем логиты в вероятности\n",
    "\n",
    "predicted_class = 1 if probability.item() >= 0.5 else 0  # Порог 0.5 для бинарной классификации\n",
    "\n",
    "# Вывод результата\n",
    "print(f'Predicted Probability: {probability.item():.4f}')\n",
    "if predicted_class ==1:\n",
    "    print(\"даун\")\n",
    "else: print(\"не даун\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d5e6b3-317e-4aec-a54e-faaa9175f43d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
